{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COEGIN\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from out/surh_convnet.chkp\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "Converted 10 variables to const ops.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 60: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1054f188703f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'out/frozen_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m#     input_graph_def.ParseFromString(f.read())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mtext_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_graph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m       return self._prepare_value(\n\u001b[1;32m--> 127\u001b[1;33m           pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m   @deprecation.deprecated_args(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m_prepare_value\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m     93\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_str_any\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \"\"\"\n\u001b[0;32m    109\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 60: invalid start byte"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.python.framework import graph_util\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import keras.models\n",
    "from keras.models import model_from_json\n",
    "from scipy.misc import imread, imresize,imshow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "json_file = open('model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#load woeights into new model\n",
    "\n",
    "# K.set_learning_phase(0)\n",
    "model = loaded_model.load_weights(\"model/model.h5\")\n",
    "sess = K.get_session()\n",
    "\n",
    "# output_node_name = \"dense_2/Softmax\" # Name of your output node\n",
    "\n",
    "# with sess as sess:\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     sess.run(init_op)\n",
    "#     graph_def = sess.graph.as_graph_def()\n",
    "#     output_graph_def = graph_util.convert_variables_to_constants(\n",
    "#                                                                  sess,\n",
    "#                                                                  sess.graph.as_graph_def(),\n",
    "#                                                                  output_node_name.split(\",\"))\n",
    "#     tf.train.write_graph(output_graph_def,\n",
    "#                          logdir=\"my_dir\",\n",
    "#                          name=\"my_model.pb\",\n",
    "#                          as_text=False)\n",
    "\n",
    "# import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "\n",
    "# gd = tf.GraphDef()\n",
    "\n",
    "# with tf.gfile.FastGFile(\"my_dir/my_model.pb\", \"r\") as f:\n",
    "#     text_format.Merge(f.read(), gd)\n",
    "\n",
    "# for node in gd.node:\n",
    "#     if node.op == \"Switch\":\n",
    "#         node.op = \"Identity\"\n",
    "#         del node.input[1]\n",
    "\n",
    "# tf.train.write_graph(gd, \".\", \"my_dir/fixed_model.pb\")\n",
    "\n",
    "MODEL_NAME = 'surh_convnet'\n",
    "saver = tf.train.Saver()\n",
    "input_node_names = [\"conv2d_1_input\"]\n",
    "output_node_name = \"dense_2/Softmax\"\n",
    "\n",
    "\n",
    "tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "                     MODEL_NAME + '_graph.pbtxt', as_text=False)\n",
    "\n",
    "saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        True, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"r\") as f:\n",
    "#     input_graph_def.ParseFromString(f.read())\n",
    "    text_format.Merge(f.read(), input_graph_def)\n",
    "\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "print(\"graph saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from google.protobuf import text_format\n",
    "# from tensorflow.python.platform import gfile\n",
    "\n",
    "# def pbtxt_to_graphdef(filename):\n",
    "#   with open(filename, 'r') as f:\n",
    "#     graph_def = tf.GraphDef()\n",
    "#     file_content = f.read()\n",
    "#     text_format.Merge(file_content, graph_def)\n",
    "#     tf.import_graph_def(graph_def, name='')\n",
    "#     tf.train.write_graph(graph_def, 'my_dir/', 'protobuf.pb', as_text=False)\n",
    "\n",
    "# def graphdef_to_pbtxt(filename): \n",
    "#   with gfile.FastGFile(filename,'rb') as f:\n",
    "#     graph_def = tf.GraphDef()\n",
    "#     graph_def.ParseFromString(f.read())\n",
    "#     tf.import_graph_def(graph_def, name='')\n",
    "#     tf.train.write_graph(graph_def, 'my_dir/', 'protobuf.pbtxt', as_text=True)\n",
    "#   return\n",
    "\n",
    "\n",
    "# graphdef_to_pbtxt('graph.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
