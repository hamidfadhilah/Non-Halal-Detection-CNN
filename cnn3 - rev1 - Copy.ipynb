{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COEGIN\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import helpers\n",
    "# import brewer2mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# set3 = brewer2mpl.get_map('Set3', 'qualitative', 4).mpl_colors\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n",
      "\n",
      "Data has been loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COEGIN\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data latih : 56581\n",
      "Data Uji :  14146 \n",
      "\n",
      "Total Data :  70727\n",
      "[[1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [0.11764706 0.18039216 0.1254902  ... 0.19215687 0.14117648 0.18039216]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [0.9843137  1.         0.95686275 ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_classes = 36\n",
    "epochs = 50\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "print('Start loading data...\\n')\n",
    "files, labels = helpers.load_chars74k_data()\n",
    "X, y = helpers.create_dataset(files, labels)\n",
    "print('Data has been loaded')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=2, train_size=0.8)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train/=255.\n",
    "x_test/=255.\n",
    "\n",
    "print('Data latih :', x_train.shape[0])\n",
    "print('Data Uji : ', x_test.shape[0],'\\n')\n",
    "\n",
    "print('Total Data : ', x_train.shape[0]+ x_test.shape[0])\n",
    "\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56581, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# train_generator, validation_generator = helpers.create_datagenerator(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# Convolutional network will be build with Keras.\n",
    "print('Start training the model.')\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, \\\n",
    "        padding='same', activation='relu', \\\n",
    "        input_shape=[28, 28, 1]))\n",
    "# 28*28*64\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "# 14*14*64\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, strides=1, \\\n",
    "        padding='same', activation='relu'))\n",
    "# 14*14*128\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "# 7*7*128\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, strides=1, \\\n",
    "        padding='same', activation='relu'))\n",
    "# 7*7*256\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "# 4*4*256\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(36, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='model/modelchars28.h5', verbose=2, save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())  \n",
    "\n",
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "\n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "# summarize history for loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"SGD\")\n",
    " \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()  \n",
    "\n",
    "# Calculate loss and accuracy.\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Model has been trained.')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy: ', score[1]*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_filters(layer, x, y):\n",
    "#     filters = layer.W.get_value()\n",
    "#     fig = plt.figure(figsize=(8, 4))\n",
    "#     for j in range(len(filters)):\n",
    "#         ax = fig.add_subplot(y, x, j+1)\n",
    "#         ax.matshow(filters[j][0], cmap = matplotlib.cm.Blues)\n",
    "#         plt.xticks(np.array([]))\n",
    "#         plt.yticks(np.array([]))\n",
    "#     plt.tight_layout()\n",
    "#     return plt\n",
    "\n",
    "# plot_filters(model.layers[0], 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print (\"Layer\", i, \"\\t\", layer.name, \"\\t\\t\", layer.input_shape, \"\\t\\t\\t\", layer.output_shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "\n",
    "namaModel = \"model.h5\"\n",
    "model.save_weights(namaModel)\n",
    "print(\"Model : \" + namaModel + \" telah disimpan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_test, batch_size=batch_size, verbose=0)\n",
    "y_pred = [np.argmax(prob) for prob in y_prob]\n",
    "y_true = [np.argmax(true) for true in y_test]\n",
    "\n",
    "print('jumlah data uji :', len(y_prob))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['-----0-----', '-----1-----', '-----2-----', '-----3-----', '-----4-----', '-----5-----', '-----6-----', '-----7-----',\n",
    "         '-----8-----', '-----9-----', '-----a-----', '-----b-----', '-----c-----', '-----d-----', '-----e-----', '-----f-----',\n",
    "         '-----g-----', '-----h-----', '-----i-----', '-----j-----', '-----k-----', '-----l-----', '-----m-----', '-----n-----', \n",
    "         '-----o-----', '-----p-----', '-----q-----', '-----r-----', '-----s-----', '-----t-----', '-----u-----', '-----v-----', \n",
    "         '-----w-----', '-----x-----', '-----y-----', '-----z-----']\n",
    "\n",
    "def class_precision(y_true, y_pred, character):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    i = [i for i, label in enumerate(labels) if label == character][0]\n",
    "    col = [cm[j,i] for j in range(0,36)]\n",
    "    return float(col[i])/sum(col)\n",
    "\n",
    "def class_recall(y_true, y_pred, character):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    i = [i for i, label in enumerate(labels) if label == character][0]\n",
    "    row = [cm[i,j] for j in range(0,36)]\n",
    "    return float(row[i])/sum(row)\n",
    "\n",
    "def class_accuracy(y_true, y_pred, character):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    i = [i for i, label in enumerate(labels) if label == character][0]\n",
    "    tp = cm[i,i]\n",
    "    fn = sum([cm[i,j] for j in range(0,36) if j != i])\n",
    "    fp = sum([cm[j,i] for j in range(0,36) if j != i])\n",
    "    tn = sum([cm[i,j] for j in range(0,36) for i in range(0,4)]) -(tp+fp+fn)\n",
    "    return float(tp + tn)/sum([tp, fn, fp, tn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in labels:\n",
    "    print(character.upper())\n",
    "    print('   acc = {0:.2f}'.format(class_accuracy(y_true, y_pred, character)))\n",
    "    print('  prec = {0:.2f}'.format(class_precision(y_true, y_pred, character)))\n",
    "    print('recall = {0:.2f}\\n'.format(class_recall(y_true, y_pred, character)))\n",
    "    \n",
    "print('===============================================================')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_train, batch_size=batch_size, verbose=0)\n",
    "y_pred = [np.argmax(prob) for prob in y_prob]\n",
    "y_true = [np.argmax(true) for true in y_train]\n",
    "\n",
    "print('jumlah data latih :', len(y_prob))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in labels:\n",
    "    print(character.upper())\n",
    "    print('   acc = {0:.2f}'.format(class_accuracy(y_true, y_pred, character)))\n",
    "    print('  prec = {0:.2f}'.format(class_precision(y_true, y_pred, character)))\n",
    "    print('recall = {0:.2f}\\n'.format(class_recall(y_true, y_pred, character)))\n",
    "    \n",
    "print('===============================================================')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.set_learning_phase(0)\n",
    "# sess = K.get_session()\n",
    "\n",
    "# output_node_name = \"dense_2/Softmax\" # Name of your output node\n",
    "\n",
    "# with sess as sess:\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     sess.run(init_op)\n",
    "#     graph_def = sess.graph.as_graph_def()\n",
    "#     output_graph_def = graph_util.convert_variables_to_constants(\n",
    "#                                                                  sess,\n",
    "#                                                                  sess.graph.as_graph_def(),\n",
    "#                                                                  output_node_name.split(\",\"))\n",
    "#     tf.train.write_graph(output_graph_def,\n",
    "#                          logdir=\"model\",\n",
    "#                          name=\"my_model.pb\",\n",
    "#                          as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
